# 010 구글 뉴스 클리핑하기
# requests 모듈을 불러온다. 그리고 bs4(beautifulsoup4) 라이브러리에서 BeautifulSoup 클래스를 불러온다
import requests, re
from bs4 import BeautifulSoup
import sys
import io
import os

currentpath = os.getcwd()
print(currentpath)

sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding = 'utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.detach(), encoding = 'utf-8')

# 구글 뉴스 검색 (검색어: 폭우)
# 구글 뉴스 웹 페이지의 루트 디렉터리 URL을 base_url에 저장한다
base_url = "https://news.google.com"

# 웹 브라우저 주소창에서 검색어 "폭우"에 대한 검색 결과 페이지의 웹 주소를 확인한다. 웹 주소를 스파이터 IDE로 복사한다
search_url = base_url + "/search?q=폭우&hl=ko&gl=KR&ceid=KR%3Ako"
resp = requests.get(search_url)
# 웹 서버로부터 응 답은 웹 페이지 객체의 text 속성에서 HTML 소스코드를 추출한다. Beautiful Soup 함수에 "html.parser" 파서를 적용하여 HTMl을 파싱하고, 변수 soup에 저장한다 
html_src = resp.text
soup = BeautifulSoup(html_src, 'html.parser')

# 뉴스 아이템 블록을 선택 
# 웹 페이지 에서 뉴스 콘텐츠 요소를 검색하기 위해, soup 객체에 select() 메소드를 적용한다. <div> 태그 중에서 class 속성이 "xrnccd"인 태그를 모두 찾아서 리스트에 담는다. new_items라는 변수에 저장하고, 원소의 개수를 len() 함수로 확인하면 99개이다. 
# 첫번째 원소에 들어 있는 <div> 태그의 내용을 출력하여 확인한다 
news_items = soup.select('div[class="xrnccd"]')
print(len(news_items))
print(news_items[0])
print("\n")

# 각 뉴스 아이템에서 "링크, 제목, 내용, 출처, 등록이시" 데이터를 파싱(BeautifulSoup)

# new_items에 들어 있는 99개의 <div> 태그를 하나식 파싱하기 위하여 for 반복문을 이용한다. 출력량을 제한하기 위해서 앞에서 3개의 원소만을 대상으로 반복문을 적용해본다.
for item in news_items[:3]:
    # 개별 뉴스의 링크는 class 속성이 "VDXfz"인 <a> 태그에 들어 있다. <a> 태그의 'href' 속성을 따로 추출하기 위해 get() 메소드를 이용한다.
    link = item.find('a', attrs= {'class':'VDXfz'}).get('href') 
    # base_url과 결합하여 출력 내용을 확인한다. link 변수의 문자열은 "./articles/~~~"과 같이 "."으로 시작하기 때문에, 첫 번째 문자(".")을 제거하기 위해 link[1:] 형식으로 문자열의 두 번째 문자부터 슬라이싱하여 선택한다.
    news_link = base_url + link[1:]
    print(news_link)
    
    # 개별 뉴스의 제목은 <a> 태그 중에서 class 속성이 "DY5T1d" 인 경우에 들어 있다. find() 명령으로 찾은 <a> 태그 요소에 getText() 메소드를 적용하여 텍스트 부분을 추출한다 
    news_title = item.find('a', attrs={'class':'DY5T1d'}).getText()
    print(news_title)
    
    # 개별 뉴스의 출처(언론사)는 <a> 태그 중에서 class 속성이 "wEwyrc AVN2gc WfKKme" 인 경우에 들어 있다. find() 명령으로 찾은 <a> 태그 요소에 getText() 메소드를 적용하여 텍스트 부분을 추출한다 
    news_agency = item.find('a', attrs={'class':'wEwyrc AVN2gc WfKKme '})
    print(news_agency)    
    
    # 개별 뉴스를 등록한 시간을 추출한다. <time> 태그 중에서 class 속성이 "WW6dff uQIVzc Sksgp slhocf"인 경우를 찾고, get() 함수로 datetime 속성값을 확인한다. split() 메소드로 문자열의 날짜와 시간 부분을 나눈다
    news_reporting = item.find('time', attrs={'class':'WW6dff uQIVzc Sksgp slhocf'})
    news_reporting_datetime = news_reporting.get('datetime').split('T')
    news_reporting_date = news_reporting_datetime[0]
    news_reporting_time = news_reporting_datetime[1][:-1]
    print(news_reporting_date, news_reporting_time)
    print("\n")
    

# 앞 코드를 이용해서 구글 뉴스 클리핑 함수 정의 
def google_news_clipping(url, limit=5):
    
    resp = requests.get(search_url)
    html_src = resp.text
    soup = BeautifulSoup(html_src, 'html.parser')
    
    news_items = soup.select('div[class="xrnccd"]')
    
    links = []; titles = []; agencies = []; reporting_dates = []; reporting_times = []; 
    
    for items in news_items[:limit]:
        link = item.find('a', attrs= {'class':'VDXfz'}).get('href') 
        news_link = base_url + link[1:]
        links.append(news_link)
        
        news_title = item.find('a', attrs={'class':'DY5T1d'}).getText()
        titles.append(news_title)
        
        news_agency = item.find('a', attrs={'class':'wEwyrc AVN2gc WfKKme '})
        agencies.append(news_agency)
        
        news_reporting = item.find('time', attrs={'class':'WW6dff uQIVzc Sksgp slhocf'})
        news_reporting_datetime = news_reporting.get('datetime').split('T')
        news_reporting_date = news_reporting_datetime[0]
        news_reporting_time = news_reporting_datetime[1][:-1]
        reporting_dates.append(news_reporting_date)
        reporting_times.append(news_reporting_time)
    
    result = {'link':links, 'title':titles, 'agency':agencies,'data':reporting_dates, 'time':reporting_times}
    
    return result 

news = google_news_clipping(search_url, 2)
print(news)
