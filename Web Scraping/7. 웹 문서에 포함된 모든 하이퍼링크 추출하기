# 007 웹 문서에 포함된 모든 하이퍼링크 추출하기
# requests 모듈을 불러온다. 그리고 bs4(beautifulsoup4) 라이브러리에서 BeautifulSoup 클래스를 불러온다
import requests, re
from bs4 import BeautifulSoup
import sys
import io
import os

currentpath = os.getcwd()
print(currentpath)

# 응답 객체의 text 속성에서 HTML 소스코드를 추출하여 변수 html_src에 할당한다
url = "https://en.wikipedia.org/wiki/Seoul_Metropolitan_Subway"
resp = requests.get(url)
html_src = resp.text

# HTML 소스코드를 파싱하여 BeautifulSoup 객체를 생성하고 변수 soup에 할당한다.
soup = BeautifulSoup(html_src, 'html.parser')


# BeautifulSoup의 find_all() 명령에 찾고자 하는 태그 이름을 매개 변수로 전달하면, 웹문서에 대항하는 모든 태그를 찾아서 리스트형태로 리턴한다. 따라서 soup 객체에 들어 있는 모든 <a> 태그를 찾고 변수 links에 저장한다.
# len() 함수로 확인하면 links 변수의 원소 개수는 모두 1219개 이다. 웸 문서 안에 <a> 태그가 1219개 있다는 뜻이.
links = soup.find_all("a")
print("하이퍼링크의 개수: ", len(links))
print("\n")
# 첫 3개의 원소를 슬라이싱으로 선택하는 내용이다.
print("첫 3개의 원소: ", links[:3])
print("\n")

# <a> 태그의 href 속성이 포함하는 문자열을 따로 지정하면, 해당 문자열이 포함된 <a> 태그만을 찾는다. 예제에서는 출력 길이를 제한하기 위하여 limit 매개변수를 3으로 설정한다. 따라서 "/wiki/" 문자열이 링크에 포함되어 있는 <a> 태그를 3개 찾아서 변수 wiki_links에 저장한다
wiki_links = soup.find_all(name="a", href = re.compile("/wiki/"), limit=3)
print("/wiki/ 문자열이 포함된 하이퍼링크: ", wiki_links)
print("\n")

# find_all() 메소드의 attrs 매개변수에 {'속성 이름': '속성값'}의 딕셔너리 형태로 찾으려는 태그나 갖는 속성값을 지정할 수 있다. class 속성값이 "external text"인 <a> 태그를 모두 찾아서 리스트 형태로 리턴한다. limit 매개변수를 3으로 설정하여 3개 까지 추출한다. 
external_links = soup.find_all(name="a", attrs={"class":"external text"}, limit=3)
print("class 속성으로 추출한 하이퍼링크: ", external_links)
